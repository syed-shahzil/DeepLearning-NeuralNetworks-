{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2624313",
   "metadata": {},
   "source": [
    "## **<span style=\"color:blue\">University of Management & Technology</span>**\n",
    "### <span>Deep Learning & Neural Networks</span>\n",
    "### Assignment 2\n",
    "\n",
    "<table style=\"width:100%; border-collapse:collapse; margin-top:20px; font-family:Arial, sans-serif; border:1px solid #bbb;\">\n",
    "\n",
    "  <tr>\n",
    "    <th colspan=\"2\" \n",
    "        style=\"background:#0056b3; color:white; text-align:left; \n",
    "               padding:10px; font-size:18px; font-weight:bold; \n",
    "               border-bottom:1px solid #bbb;\">\n",
    "      Information\n",
    "    </th>\n",
    "  </tr>\n",
    "\n",
    "  <tr>\n",
    "    <td style=\"padding:10px; font-weight:bold; width:30%;\">Student Name:</td>\n",
    "    <td style=\"padding:10px;\">Syed Shahzil Abbas</td>\n",
    "  </tr>\n",
    "\n",
    "  <tr>\n",
    "    <td style=\"padding:10px; font-weight:bold;\">ID:</td>\n",
    "    <td style=\"padding:10px;\">23018020020</td>\n",
    "  </tr>\n",
    "\n",
    "  <tr>\n",
    "    <td style=\"padding:10px; font-weight:bold;\">Program:</td>\n",
    "    <td style=\"padding:10px;\">BSCS Sec A Batch 18 â€” Semester Fall 25</td>\n",
    "  </tr>\n",
    "\n",
    "  <tr>\n",
    "    <td style=\"padding:10px; font-weight:bold;\">Course Title:</td>\n",
    "    <td style=\"padding:10px;\">Deep Learning and Neural Networks</td>\n",
    "  </tr>\n",
    "\n",
    "  <tr>\n",
    "    <td style=\"padding:10px; font-weight:bold;\">Resource Person:</td>\n",
    "    <td style=\"padding:10px;\">Ms Hina Tufail</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061f6a25",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d2160a0a",
   "metadata": {},
   "source": [
    "**_<spam style=\"color:blue\">KeyPoints:</span>_**\n",
    "- Load the same dataset used in Logistic Regression.\n",
    "- init_param(n_x, n_h, n_y).\n",
    "- Activation functions: sigmoid, relu.\n",
    "- Forward Propagation. \n",
    "- Cost Function.\n",
    "- Backword Propagation.\n",
    "- Fit (Gradient Descent).\n",
    "- Analysis Report \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9292e5d5",
   "metadata": {},
   "source": [
    "**_Loading and Spliting Dataset_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f15262ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "976c17ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"Bank_Customer_Churn_Prediction_Processed.csv\")\n",
    "X =data[['country_Spain','country_Germany','gender_Male','credit_card','active_member','age','tenure','credit_score','balance','products_number','estimated_salary']]\n",
    "Y = data['churn']\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "11e3d2d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (8000, 11)\n",
      "Y_train shape: (8000,)\n",
      "X_test shape: (2000, 11)\n",
      "Y_test shape: (2000,)\n",
      "Number of training examples: 8000\n",
      "Number of testing examples: 2000\n"
     ]
    }
   ],
   "source": [
    "# Shape of inputs\n",
    "xtrain_shape =X_train.shape\n",
    "ytrain_shape =Y_train.shape\n",
    "xtest_shape  =X_test.shape\n",
    "ytest_shape  =Y_test.shape\n",
    "m_train = X_train.shape[0]\n",
    "m_test = X_test.shape[0]\n",
    "\n",
    "print(\"X_train shape:\", xtrain_shape)\n",
    "print(\"Y_train shape:\", ytrain_shape)\n",
    "print(\"X_test shape:\", xtest_shape)\n",
    "print(\"Y_test shape:\", ytest_shape)\n",
    "print(\"Number of training examples:\", m_train)\n",
    "print(\"Number of testing examples:\", m_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "feb3213f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer Sizes\n",
    "n_x = X_train.shape[1] \n",
    "n_h = 4                 \n",
    "n_y = 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb31cac",
   "metadata": {},
   "source": [
    "**_Initialize Parameters_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8e4fada0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    n_x -- size of the input layer\n",
    "    n_h -- size of the hidden layer\n",
    "    n_y -- size of the output layer\n",
    "    \n",
    "    Returns:\n",
    "    params -- python dictionary containing your parameters:\n",
    "                    W1 -- weight matrix of shape (n_h, n_x)\n",
    "                    b1 -- bias vector of shape (n_h, 1)\n",
    "                    W2 -- weight matrix of shape (n_y, n_h)\n",
    "                    b2 -- bias vector of shape (n_y, 1)\n",
    "    \"\"\"    \n",
    "    W1=np.random.randn(n_h,n_x) *0.01\n",
    "    W2=np.random.randn(n_y,n_h) *0.01\n",
    "    b1=np.zeros((n_h,1))\n",
    "    b2=np.zeros((n_y,1))\n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3424af",
   "metadata": {},
   "source": [
    "**_Activation Function:_**\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "$$\n",
    "\\text{ReLU}(z) = \\max(0, z)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b0942567",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    z = np.clip(z, -500, 500)\n",
    "    s = 1/(1+np.exp(-z))\n",
    "    return s\n",
    "def relu(z):\n",
    "    r = np.maximum(0,z)\n",
    "    return r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016d73b1",
   "metadata": {},
   "source": [
    "**_Forward Propagation:_**\n",
    "\n",
    "$$\n",
    "Z^{[1]} = W^{[1]} X + b^{[1]} \\tag{1}\n",
    "$$\n",
    "$$\n",
    "A^{[1]} = \\text{ReLU}\\left(Z^{[1]}\\right) \\tag{2}\n",
    "$$\n",
    "$$Z^{[2]} = W^{[2]} A^{[1]} + b^{[2]}\\tag{3}$$\n",
    "$$\\hat{Y} = A^{[2]} = \\sigma(Z^{[2]})\\tag{4}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a1b6cafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(x, parameters):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    x -- input data of size (n_x, m)\n",
    "    parameters -- python dictionary containing your parameters (output of initialization function)\n",
    "    \n",
    "    Returns:\n",
    "    A2 -- The sigmoid output of the second activation\n",
    "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\"\n",
    "    \"\"\"\n",
    "    W1=parameters[\"W1\"]\n",
    "    b1=parameters[\"b1\"]\n",
    "    W2=parameters[\"W2\"]\n",
    "    b2=parameters[\"b2\"]\n",
    "    # LAYER 1\n",
    "    Z1=np.dot(W1,x)+b1\n",
    "    A1=relu(Z1)\n",
    "    # LAYER 2\n",
    "    Z2=np.dot(W2,A1)+b2\n",
    "    A2=sigmoid(Z2)\n",
    "    \n",
    "    cache = {\"Z1\": Z1,\n",
    "             \"A1\": A1,\n",
    "             \"Z2\": Z2,\n",
    "             \"A2\": A2}\n",
    "    \n",
    "    return A2, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9923916e",
   "metadata": {},
   "source": [
    "**_Cost Function_**\n",
    "- Cross Entropy \n",
    "$$J = - \\frac{1}{m} \\sum\\limits_{i = 1}^{m} \\large{(} \\small y^{(i)}\\log\\left(a^{[2] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[2] (i)}\\right) \\large{)} \\small$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ce07efd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(A2, y):\n",
    "    \"\"\"\n",
    "    Computes the cross-entropy cost given in equation (13)\n",
    "    \n",
    "    Arguments:\n",
    "    A2 -- The sigmoid output of the second activation, of shape (1, number of examples)\n",
    "    y -- \"true\" labels vector of shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost given equation (13)\n",
    "    \n",
    "    \"\"\"\n",
    "    if(y.ndim == 1):\n",
    "        m = y.shape[0] # number of examples\n",
    "    else:\n",
    "        m = y.shape[1] # number of examples\n",
    "    logprobs=np.multiply(np.log(A2),y)+np.multiply(np.log(1-A2),(1-y))\n",
    "    cost= -np.sum(logprobs)/m\n",
    "    \n",
    "    cost = float(np.squeeze(cost))   \n",
    "                                    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c3d997",
   "metadata": {},
   "source": [
    "**_Backword Propagation_**\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{J} }{ \\partial z_{2}^{(i)} } = \\frac{1}{m} (a^{[2](i)} - y^{(i)})$$\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{J} }{ \\partial W_2 } = \\frac{\\partial \\mathcal{J} }{ \\partial z_{2}^{(i)} } a^{[1] (i) T} $$\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{J} }{ \\partial b_2 } = \\sum_i{\\frac{\\partial \\mathcal{J} }{ \\partial z_{2}^{(i)}}}$$\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{J} }{ \\partial z_{1}^{(i)} } =  W_2^T \\frac{\\partial \\mathcal{J} }{ \\partial z_{2}^{(i)} } * ( 1 - a^{[1] (i) 2}) $$\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{J} }{ \\partial W_1 } = \\frac{\\partial \\mathcal{J} }{ \\partial z_{1}^{(i)} }  X^T $$\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{J} _i }{ \\partial b_1 } = \\sum_i{\\frac{\\partial \\mathcal{J} }{ \\partial z_{1}^{(i)}}}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4c0bad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4d31ce3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(parameters, cache, x, y):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation using the instructions above.\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing our parameters \n",
    "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\".\n",
    "    x -- input data of shape (2, number of examples)\n",
    "    y -- \"true\" labels vector of shape (1, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    grads -- python dictionary containing your gradients with respect to different parameters\n",
    "    \"\"\"\n",
    "    m = x.shape[1]\n",
    "    \n",
    "    W1=parameters[\"W1\"]\n",
    "    W2=parameters[\"W2\"]\n",
    "        \n",
    "    A1=cache[\"A1\"]\n",
    "    A2=cache[\"A2\"]\n",
    "    \n",
    "    dZ2=A2 - y\n",
    "    dW2=(1/m)*np.dot(dZ2,A1.T)\n",
    "    db2=(1/m)*np.sum(dZ2,axis=1,keepdims=True)\n",
    "    dZ1=np.dot(W2.T,dZ2)*(1 - np.power(A1,2))\n",
    "    dW1=(1/m)*np.dot(dZ1,x.T)\n",
    "    db1=(1/m)*np.sum(dZ1,axis=1,keepdims=True)\n",
    "    \n",
    "    grads = {\"dW1\": dW1,\n",
    "             \"db1\": db1,\n",
    "             \"dW2\": dW2,\n",
    "             \"db2\": db2}\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff86cb4",
   "metadata": {},
   "source": [
    "**_Fit Function_**\n",
    "- Performs gradient descent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "68a317e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate = 1.2):\n",
    "    \"\"\"\n",
    "    Updates parameters using the gradient descent update rule given above\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients \n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "    \"\"\"\n",
    "    W1=parameters[\"W1\"]\n",
    "    b1=parameters[\"b1\"]\n",
    "    W2=parameters[\"W2\"]\n",
    "    b2=parameters[\"b2\"]\n",
    "    \n",
    "    dW1=grads[\"dW1\"]\n",
    "    db1=grads[\"db1\"]\n",
    "    dW2=grads[\"dW2\"]\n",
    "    db2=grads[\"db2\"]\n",
    "    \n",
    "    W1=W1 - learning_rate * dW1\n",
    "    b1=b1 - learning_rate * db1\n",
    "    W2=W2 - learning_rate * dW2\n",
    "    b2=b2 - learning_rate * db2\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "96df555a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(x, y, n_h, l_r, num_iterations = 10000, print_cost=False):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    x -- dataset of shape (2, number of examples)\n",
    "    y -- labels of shape (1, number of examples)\n",
    "    n_h -- size of the hidden layer\n",
    "    l_r -- learning rate\n",
    "    num_iterations -- Number of iterations in gradient descent loop\n",
    "    print_cost -- if True, print the cost every 1000 iterations\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "\n",
    "    n_x = x.shape[0]\n",
    "    n_y = 1  \n",
    "    parameters=initialize_parameters(n_x,n_h,n_y)\n",
    "    \n",
    "    for i in range(0, num_iterations):\n",
    "        A2,cache=forward_propagation(x,parameters)\n",
    "        cost=compute_cost(A2,y)\n",
    "        grads=backward_propagation(parameters,cache,x,y)\n",
    "        parameters=update_parameters(parameters,grads, l_r)\n",
    "        \n",
    "        if print_cost and i % 1000 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4416b23",
   "metadata": {},
   "source": [
    "**Analytics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "23d122e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.693181\n",
      "Cost after iteration 1000: 0.513927\n",
      "Cost after iteration 2000: 0.510031\n",
      "Cost after iteration 3000: 0.505716\n",
      "Cost after iteration 4000: 0.488122\n",
      "Cost after iteration 5000: 0.461681\n",
      "Cost after iteration 6000: 0.452370\n",
      "Cost after iteration 7000: 0.449463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\F1C\\AppData\\Local\\Temp\\ipykernel_20232\\2426020177.py:17: RuntimeWarning: divide by zero encountered in log\n",
      "  logprobs=np.multiply(np.log(A2),y)+np.multiply(np.log(1-A2),(1-y))\n",
      "C:\\Users\\F1C\\AppData\\Local\\Temp\\ipykernel_20232\\2426020177.py:17: RuntimeWarning: invalid value encountered in multiply\n",
      "  logprobs=np.multiply(np.log(A2),y)+np.multiply(np.log(1-A2),(1-y))\n",
      "C:\\Users\\F1C\\AppData\\Local\\Temp\\ipykernel_20232\\2426020177.py:17: RuntimeWarning: divide by zero encountered in log\n",
      "  logprobs=np.multiply(np.log(A2),y)+np.multiply(np.log(1-A2),(1-y))\n",
      "C:\\Users\\F1C\\AppData\\Local\\Temp\\ipykernel_20232\\2426020177.py:17: RuntimeWarning: invalid value encountered in multiply\n",
      "  logprobs=np.multiply(np.log(A2),y)+np.multiply(np.log(1-A2),(1-y))\n",
      "C:\\Users\\F1C\\AppData\\Local\\Temp\\ipykernel_20232\\2426020177.py:17: RuntimeWarning: divide by zero encountered in log\n",
      "  logprobs=np.multiply(np.log(A2),y)+np.multiply(np.log(1-A2),(1-y))\n",
      "C:\\Users\\F1C\\AppData\\Local\\Temp\\ipykernel_20232\\2426020177.py:17: RuntimeWarning: invalid value encountered in multiply\n",
      "  logprobs=np.multiply(np.log(A2),y)+np.multiply(np.log(1-A2),(1-y))\n",
      "C:\\Users\\F1C\\AppData\\Local\\Temp\\ipykernel_20232\\2426020177.py:17: RuntimeWarning: divide by zero encountered in log\n",
      "  logprobs=np.multiply(np.log(A2),y)+np.multiply(np.log(1-A2),(1-y))\n",
      "C:\\Users\\F1C\\AppData\\Local\\Temp\\ipykernel_20232\\2426020177.py:17: RuntimeWarning: invalid value encountered in multiply\n",
      "  logprobs=np.multiply(np.log(A2),y)+np.multiply(np.log(1-A2),(1-y))\n",
      "C:\\Users\\F1C\\AppData\\Local\\Temp\\ipykernel_20232\\2426020177.py:17: RuntimeWarning: divide by zero encountered in log\n",
      "  logprobs=np.multiply(np.log(A2),y)+np.multiply(np.log(1-A2),(1-y))\n",
      "C:\\Users\\F1C\\AppData\\Local\\Temp\\ipykernel_20232\\2426020177.py:17: RuntimeWarning: invalid value encountered in multiply\n",
      "  logprobs=np.multiply(np.log(A2),y)+np.multiply(np.log(1-A2),(1-y))\n",
      "C:\\Users\\F1C\\AppData\\Local\\Temp\\ipykernel_20232\\3858013449.py:19: RuntimeWarning: overflow encountered in dot\n",
      "  Z2=np.dot(W2,A1)+b2\n",
      "C:\\Users\\F1C\\AppData\\Local\\Temp\\ipykernel_20232\\814529668.py:25: RuntimeWarning: overflow encountered in power\n",
      "  dZ1=np.dot(W2.T,dZ2)*(1 - np.power(A1,2))\n",
      "C:\\Users\\F1C\\AppData\\Local\\Temp\\ipykernel_20232\\814529668.py:26: RuntimeWarning: invalid value encountered in dot\n",
      "  dW1=(1/m)*np.dot(dZ1,x.T)\n",
      "c:\\Users\\F1C\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:86: RuntimeWarning: invalid value encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 8000: nan\n"
     ]
    }
   ],
   "source": [
    "parameters = fit(X_train.T, Y_train.values.reshape(1, -1), n_h = 4, l_r=0.01 ,num_iterations = 9000, print_cost=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1caa2f24",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m y_hat, cache =forward_propagation(X_train.T,parameters)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m y_hat= \u001b[32m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43my_hat\u001b[49m\u001b[43m>\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.5\u001b[39;49m \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(y_hat) \n",
      "\u001b[31mValueError\u001b[39m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "y_hat, cache =forward_propagation(X_train.T,parameters)\n",
    "y_hat= 1 if y_hat>=0.5 else 0\n",
    "print(y_hat) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8e08bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
